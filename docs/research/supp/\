---
title: Supplementary
type: docs
bookToc: false
---

# Supplementary Materials

<link rel="stylesheet" href="/academicons/academicons-1.9.0/css/academicons.min.css"/>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
<head>
<script src='https://kit.fontawesome.com/a076d05399.js' crossorigin='anonymous'></script>
<link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

This website maintains the supplementary materials related to the following paper:

{{< hint info >}}
**<ins>Ke Li</ins>**, Han Guo+, "Human-in-the-Loop Policy Optimization for Preference-Based Multi-Objective Reinforcement Learning", submitted for peer review, 2024.
{{< /hint >}}

It consists of the following parts:
- The source code of this paper can be found from our <i class='fa fa-github-square' style='font-size:16px'></i> [Github repo](https://github.com/COLA-Laboratory/ranknet).
- Appendix document of this paper can be found from this <i class='fa fa-dropbox' style='font-size:16px'></i> [Dropbox link](https://www.dropbox.com/s/77bq1zooeebuyp8/supp.pdf?dl=0).
- Video clips of the preferred policy obtained by our proposed PBMORL. In particular, {{< katex >}}f_1{{< /katex >}} is the x-axis speed, {{< katex >}}f_2{{< /katex >}} is the y-axis speed, {{< katex >}}f_3 \text{ is the energy consumption} {{< /katex >}}
<div align="left">
Ant<br>
<img src="/media/gifs/pbmorl/antprefer1.gif" width = "200px" align=center title="Moving peak benchmark" />
<img src="/media/gifs/pbmorl/antprefer2.gif" width = "200px" align=center title="Moving peak benchmark" />
<img src="/media/gifs/pbmorl/halfcheetahprefer1.gif" width = "200px" align=center title="Moving peak benchmark" />
<img src="/media/gifs/pbmorl/halfcheetahprefer2.gif" width = "200px" align=center title="Moving peak benchmark" /><br>

Swimmer<br>
<img src="/media/gifs/pbmorl/swimmerprefer1.gif" width = "200px" align=center title="Moving peak benchmark" />
<img src="/media/gifs/pbmorl/swimmerprefer2.gif" width = "200px" align=center title="Moving peak benchmark" />
<img src="/media/gifs/pbmorl/walkerprefer1.gif" width = "200px" align=center title="Moving peak benchmark" />
<img src="/media/gifs/pbmorl/walkerprefer2.gif" width = "200px" align=center title="Moving peak benchmark" /><br>

Humanoid<br>
<img src="/media/gifs/pbmorl/humanoidprefer1.gif" width = "200px" align=center title="Moving peak benchmark" />
<img src="/media/gifs/pbmorl/humanoidprefer2.gif" width = "200px" align=center title="Moving peak benchmark" />
<img src="/media/gifs/pbmorl/hopperprefer1.gif" width = "200px" align=center title="Moving peak benchmark" />
<img src="/media/gifs/pbmorl/hopperprefer2.gif" width = "200px" align=center title="Moving peak benchmark" /><br>

Hopper<br>
<img src="/media/gifs/pbmorl/hopper3prefer1.gif" width = "200px" align=center title="Moving peak benchmark with Gaussian peaks" />
<img src="/media/gifs/pbmorl/hopper3prefer2.gif" width = "200px" align=center title="Moving peak benchmark with Gaussian peaks" />
<img src="/media/gifs/pbmorl/hopper3prefer3.gif" width = "200px" align=center title="Moving peak benchmark with Gaussian peaks" />
</div>

Please cite the paper by using the following bibtex.
```
@article{LiLY22,
    author    = {Ke Li and
                 Han Guo},
    title     = {Human-in-the-Loop Policy Optimization for Preference-Based Multi-Objective Reinforcement Learning},
    journal   = {ArXiv},
    pages     = {1--15},
    year      = {2024},
    note      = {under review}
}
```
