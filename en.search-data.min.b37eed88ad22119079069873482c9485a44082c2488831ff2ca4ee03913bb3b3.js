'use strict';(function(){const t={cache:!0};t.doc={id:"id",field:["title","content"],store:["title","href","section"]};const e=FlexSearch.create("balance",t);window.bookSearchIndex=e,e.add({id:0,href:"/posts/pgfplots/",title:"How to use TikZ in scientific plotting?",section:"Blog",content:"\rTikZ is a powerful drawing package for academics who are using LaTeX for paper writing. Unlike other TeX packages, TikZ itself is really huge, its manual contains about 1100+ pages, 100+ chapters. Tens of other packages, and serveral GUI applications are based on TikZ.\nInstead of being comprehensive, this page mainly aims to provide a curated list of awesome TikZ templates frequently used in our COLA laboratory.\n"}),e.add({id:1,href:"/posts/overleaf_naming/",title:"How to name your Overleaf project?",section:"Blog",content:"\r\rOverleaf is a collaborative cloud-based LaTeX editor used for writing, editing and publishing scientific documents. In COLA laboratory, we use the Overleaf as the centralized place to manage our manuscripts. In order to make the projects be managed in an organized manner, a project is usually named as A_B_C.\n A: journal/conference name, e.g., TEVC/PPSN22 B: acronym of the algorithm, e.g., TBO2 C: first character of your name, e.g., RC  This comes up to be a typical example: TEVC_TBO2_RC.\nNote that since the University of Exeter is a partner of Overleaf, you are free to use the professional version by using your university\u0026rsquo;s email account to register. There are some very cool function of using this professional version.\n You can sync your project to either Dropbox or Github, though this is applicable for regular account. This is very useful when you find the online Overleaf version is kind of slow. It is very cool to track the changes by turning Track changes on. Try to get yourself comfortable with the comment function in Overleaf, so that we can address concerns in an interactive manner. \u0026hellip;\u0026hellip;  There can be even more cool stuffs. I will amend this later when it comes down.\n"}),e.add({id:2,href:"/posts/bibtex_rule/",title:"How to prepare your bibtex file?",section:"Blog",content:"\rBibtex is a powerful tool to manage your references in scientific writing by using LaTeX. However, the bibtex sources can be messy in the wild. Here are some useful tips to prepare a beautiful and clean bibtex file.\n Instead of searching from Google, it might be more convenient to search the corresponding paper or author in DBLP. There is an option to choose the export record as the BibTeX. It will be more efficient to use some text editor like Vim to facilitate your editing. However, there is a steep learning curve to manage the Vim. Be careful to include unnecessary components in your bib. For example, it will take extra space if you include url and doi. Note that most publishers have a strict rule on the page length. If the corresponding paper is not published yet, i.e., just accepted or in press, there is usually NO pages, volume and number related information. Instead, we just need to amend a note component in the bibtex: note={accepted for publication}.  In the following paragraphs, we mainly focus on three types of publications to give you some tangible examples.\nJournal article\nLet us use the following example from one of my papers to explain the format.\n@article{LiZKLW14,\rauthor = {Ke Li and\rQingfu Zhang and\rSam Kwong and\rMiqing Li and\rRan Wang},\rtitle = {Stable Matching-Based Selection in Evolutionary Multiobjective Optimization},\rjournal = {{IEEE} Trans. Evol. Comput.},\rvolume = {18},\rnumber = {6},\rpages = {909--923},\ryear = {2014},\rdoi = {10.1109/TEVC.2013.2293776},\rtimestamp = {Mon, 27 Nov 2017 16:55:26 +0100},\rbiburl = {http://dblp.org/rec/bib/journals/tec/LiZKLW14},\rbibsource = {dblp computer science bibliography, http://dblp.org}\r} Most components can be directly obtained from the paper itself. We only explicitly explain the following components.\n bibtex entry ABC:  A: The last name of the first author, e.g., \u0026ldquo;A = Li\u0026rdquo; in this example. B: The first letter of each of the other authors, e.g., \u0026ldquo;B = ZKLW\u0026rdquo; in this example. C: Shortcut of the publication year, e.g., \u0026ldquo;C = 14\u0026rdquo; in this example.   title: Please capitalize each word in the paper title, except for some conjunction words or so. journal: Please try to use the abbreviation of the journal name as much as you can. You can easily Google this information. Alternatively, you can refer to the relevant publisher for detailed information. pages: Please use double hyphens to connect the page numbers. doi, timestamp, biburl and bibsource are not always necessary. But you can find more information from the DBLP of the corresponding author(s).  If the paper is not published yet, the bibtex will be simplified as follows.\n@article{LiNGY21,\rauthor = {Ke Li and\rHaifeng Nie and\rHuiru Gao and\rXin Yao},\rtitle = {Posterior Decision-Making Based on Decomposition-Driven Knee Point Identification},\rjournal = {{IEEE} Trans. Evol. Comput.},\ryear = {2021},\rnote = {accepted for publication}\r}  Conference article\nConference paper is slightly different from the journal article. It is usually published as a proceeding thus there is no issue or volume number. Instead, we usually need to formulate the name of the proceeding. Sometimes the editors are required to be listed, but I usually recommend to omit that part. Let us use the following example to explain this format.\n@inproceedings{WuKJLZ17,\rauthor = {Mengyuan Wu and\rSam Kwong and\rYuheng Jia and\rKe Li and\rQingfu Zhang},\rtitle = {Adaptive weights generation for decomposition-based multi-objective optimization using Gaussian process regression},\rbooktitle = {GECCO\u0026#39;17: Proc. of the 2017 Genetic and Evolutionary Computation Conference},\rpages = {641--648},\rpublisher = {{ACM}},\ryear = {2017},\rtimestamp = {Fri, 27 Mar 2020 09:03:02 +0100},\rbiburl = {https://dblp.org/rec/conf/gecco/WuKJLZ17.bib},\rbibsource = {dblp computer science bibliography, https://dblp.org}\r} The other rules are almost the same as the journal article, except the following components.\n booktitle is formulated as: S'Y: Proc. of the xxxx Conference on  S: Abbreviation of the conference, e.g., \u0026ldquo;S = GECCO\u0026rdquo; in this example. Y: Shortcut of the conference year, e.g., \u0026ldquo;Y = 17\u0026rdquo; in this example. xxxx: conference year, e.g., \u0026ldquo;xxxx = 2017\u0026rdquo; in this example.     Book\nBook is a bit different from the journal and conference papers. Most components can be directly derived from the book itself. You can apply the previous formats here. Let us use the following example to explain this format.\n@book{Deb01,\rauthor = {Kalyanmoy Deb},\rtitle = {Multi-Objective Optimization Using Evolutionary Algorithms},\rpublisher = {John Wiley \\\u0026amp; Sons, Inc.},\raddress = {New York, NY, USA},\ryear = {2001}\r}  Technical report\n@techreport{ABC,\rauthor = {xx and\rxx},\rtitle = {xx},\rinstitution = {xx},\ryear = {xxxx}\r}  ArXiv paper\n@article{Frazier18,\rauthor = {Peter I. Frazier},\rtitle = {A Tutorial on {Bayesian} Optimization},\rjournal = {CoRR},\rvolume = {abs/1807.02811},\ryear = {2018},\rurl = {http://arxiv.org/abs/1807.02811},\rarchivePrefix = {arXiv},\reprint = {1807.02811},\rtimestamp = {Mon, 13 Aug 2018 16:48:03 +0200},\rbiburl = {https://dblp.org/rec/journals/corr/abs-1807-02811.bib},\rbibsource = {dblp computer science bibliography, https://dblp.org}\r} "}),e.add({id:3,href:"/academicons/academicons-1.9.0/README/",title:"Readme",section:"Academicons",content:"What is Academicons?\r#\r\rAcademicons is a specialist icon font for academics. It contains icons for websites and organisations related to academia that are often missing from mainstream font packages. It can be used by itself, but its primary purpose is to be used as a supplementary package alongside a larger icon set. Go here to view the full icon set along with instructions for their use.\nRequesting new icons\r#\r\rNew icons can be requested by creating an issue here. Before submitting a request, please check that the following conditions are satisfied:\n  The organisation in question is already using a logo/icon of appropriate dimensions (roughly square). If that doesn\u0026rsquo;t exist, then there\u0026rsquo;s really not much that can be done, and the request will have to be ignored until such time that a logo/icon can be provided.\n  An icon of appropriate resolution can be provided or linked to. Ideally, the provided file will be a vector file (e.g. SVG, EPS, AI) or a PDF with the vector file embedded. These files are all very easy to work with, and result in the most faithful reproductions of the icon. Altenatively, high resolution raster images (e.g. JPEG, PNG, GIF) can work, but only if the resolution is high enough that the underlying shapes can be reproduced. Icons made from raster images take much longer to prepare, and require hand drawing each component and figuring out the exact typeface used for any letters. This process can be rather tedious, and I will only do this if there is significant demand for the icon. Favicon files can be useful in conjunction with larger logos that have non-ideal aspect ratios—where they can indicate which part of the logo to strip down to—but they are pretty much useless by themselves. The only time I have made an icon from a favicon was for arXiv, and that was only because: (i) It was heavily requested, and (ii) I was able to get feedback on the new icon from Paul Ginsparg, who made the original icon. You can still submit the request, but it will likely be ignored until someone else comes along and provides the file we need.\n  The icon can be reduced to monochrome. This is one of the basic requirements of a versatile icon, but it is often overlooked when icons are made by people who are not professional designers. Academia is full of unprofessional designers, and it is sometimes the case that a logo relies entirely on the use of different colours. In certain cases we can be creative (see the dblp logo), but more often than not it will be impossible to create a monochrome version of the icon. Again, feel free to make the request, but it will probably be ignored if an alternate logo cannot be found.\n  License\r#\r\r The Academicons font is licensed under the SIL OFL 1.1:  \rhttp://scripts.sil.org/OFL   Academicons CSS, LESS, and SASS files are licensed under the MIT License:  \rhttp://opensource.org/licenses/mit-license.html   The Academicons documentation is licensed under the CC BY 3.0 License:  \rhttp://creativecommons.org/licenses/by/3.0/    Author\r#\r\r GitHub: https://github.com/jpswalsh Web: http://jpswalsh.com  "}),e.add({id:4,href:"/docs/research/archive_reading/",title:"Archive Reading",section:"Docs",content:"Archive of Reading Group\r#\r\rAgenda (Year 2020)\r#\r\rDec\r  Surrogate Assisted Evolutionary Algorithms, Fan Li Dec 27\n[slides]\n  RoboGrammar: Graph Grammar for Terrain-Optimized Robot Design, ACM Trans. Graphics. Jiahao Zhou Dec 20\n[\rpaper] [slides]\n  Optimization, Fast and Slow: Optimally Switching Between Local and Bayesian Optimization, ICML 2018. Guozhao Wei Dec 13\n[\rpaper] [slides]\n  Nov\r  Ordinal Regression with Multiple Output CNN for Age Estimation, CVPR 2016. Guiyu Lai Nov 29\n[\rpaper] [\rslides]\n  Towards Fast Adaptation of Neural Architectures with Meta Learning, ICLR 2020. Shasha Zhou Nov 22\n[\rpaper] [\rslides]\n  \r"}),e.add({id:5,href:"/docs/research/projects/",title:"Projects",section:"Docs",content:"Projects\r#\r\rUnder construction\r\r"}),e.add({id:6,href:"/docs/misc/ddl/",title:"Deadlines",section:"Docs",content:"Deadlines\r#\r\rHere are some important deadlines for the major conferences in artificial intelligence and machine learning (AI \u0026amp; ML), data mining (DM), natural language processing (NLP), software engineering (SE) and evolutionary computation (EC) domains.\nAI \u0026amp; ML\r \rIJCAI 2022: January 14, 2022 \rICML 2022: January 27, 2022 \rUAI 2022: February 25, 2022 \rNeurIPS 2022: May 17, 2022 \rAAAI 2023: August 15, 2022 \rICLR 2023: TBD \rAISTAS 2023: TBD  DM\r \rICDM 2022: June 2022 \rSDM 2023: TBD  NLP\r \rACL 2022: November 15, 2021 \rNAACL 2022: Jan 15, 2022 \rCOLING 2022: May 15, 2022 \rEMNLP 2022: May 24, 2022  SE\r \rISSTA 2022: Jan 28, 2022 \rFSE 2022: March 17, 2022 \rASE 2022: May 6, 2022 \rICSE 2022: September 1, 2022  EC\r \rGECCO 2022: February 3, 2022 \rWCCI 2022: January 31, 2022 \rPPSN 2022: April 13, 2022 \rIJCNN 2022: February 10, 2022 \rCEC 2022: February 21, 2022 \rSMC 2022: April 15, 2022 \rFOGA 2022: May 7, 2022 \rSSCI 2022: July 1, 2022  \r"}),e.add({id:7,href:"/docs/research/emoc/",title:"EMOC",section:"Docs",content:"\rEMOC stands for Evolutionary Multi-objective Optimization in C++ which is a framework for single-objective and multi-objective optimization.\nFor more information about installation and usage, please refer to the document site.\n Features\r#\r\r  Rich algorithms and problems\nEMOC implemented about 30 algorithms and more than 80 test problems which both include single-objective and multi-objective types.\n  Developed in C++ and Cross-platform\nAll the source code of EMOC are wrote in C++. So the running efficiency is guaranteed when comparing other implementation with python, java or matlab. We also provide cross-platform compatibility for users with different operator systems.\n  Friendly GUI\nEMOC provides a user-friendly GUI with the function of configuring parameters of once-run and experiments without writing a single line of code.\n  Various Optimization Types\nEMOC supports unconstraint optimization, constraint optimization and combinatorial optimization (including binary encoding and permutation encoding).\n  Save into Excel or Latex\nUsers can save the experiment results of EMOC in the format of Latex.\n   Third Party Libraries\r#\r\r \rGLFW \rALGLIB \rDear Imgui \rpybind11 \rcxxopts \rstb_image \rGnuplot \rCMake \rFontAwesome  "}),e.add({id:8,href:"/docs/research/grants/",title:"Grants",section:"Docs",content:"Grants\r#\r\r\r\r\r\rhr.dashed {\rborder-top: 1px dashed #bbb;\r}\r.grid-container {\rdisplay: grid;\rgrid-template-columns: 40% 60%;\rgrid-gap: 5px;\rbackground-color: transparent;\rpadding: 5px;\r}\r\rWe are grateful to have been generously supported by or closely working with the following funding bodies and industrial partners.\n Ongoing\r Transfer Bayesian Optimization for Multi-Fidelity Data in Uncertain Environments\n European Network Fund |  #GP ENF5.10 |  PI |  £7,440 |  2021\u0026ndash;2022.    Evolutionary Multi-Objective Search for Automating CNN Architecture Design\n Royal Society |  #IES\\R2\\212077 |  PI |  £11,815 |  2021\u0026ndash;2023.    Human-Centric Computing\n Alan Turing Fellowship |  PI |  2021\u0026ndash;2022.    Towards Scalable Multi-Objective Bilevel Optimization: Foundations, Methodologies and Applications\n Hong Kong GRF |  #\r11211521 |  Co-I (with PI: Prof. Kay Chen Tan from The Hong Kong Polytechnic University) |  HK$1,150,000 |  2021\u0026ndash;2024.    Many Hands Make Work Light: Multi-task Deep Semantic Learning for Testing Web Application Firewalls\n Amazon Research Award |  PI |  $80,000 +  $10,000 |  2021\u0026ndash;2022.    Knowledge Representation in Transfer Optimisation System and Applications for Highly Configurable Software Systems\n EPSRC DTP Doctoral Prize |  #2404317 |  PI |  £74,000 |  2020\u0026ndash;2024.    Transfer Optimisation System for Adaptive Automated Nature-Inspired Optimisation\n UKRI Future Leaders Fellowship |  #MR/S017062/1 |  PI |  £1,370,803 |  2019\u0026ndash;2023.    Dynamic Resource Management and Optimization for SLA Guarantees in Hyperconverged Communication Infrastructures\n EPSRC Industrial CASE Studentship |  #EP/P51049X/1 |  Co-I (with PI: Prof. Geyong Min) |  £81,430 |  2017\u0026ndash;2021.    Multi-Task Semi-Supervised Deep Learning based on Evolutionary Algorithms and Its Applications\n NSFC |  #61876162 |  Co-I (with PI: Prof. Kay Chen Tan) |  ¥640,000 |  2019\u0026ndash;2022.     University of Exeter Start-up Grant |  PI |  £10,000 |  2017\u0026ndash;present.  Finished\r Three-Fold Decomposition in Multi-objective Optimization\n North-European Associated Team Project between INRIA Lille Nord Europe and University of Exeter |  PI |  €10,000 |  2018\u0026ndash;2020.    Key Questions in Multi-Label Active Learning: Multi-objective Optimisation\n Royal Society International Exchange Program (Cost Share with NSFC) |  #IEC/NSFC/170243 |  PI |  £11,863 |  2018\u0026ndash;2020.    Research on Key Problems in Dynamic Environment Multi-objective Evolutionary Optimization\n NSFC |  #61502408 |  Co-I (with PI: Prof. Juan Zou) |  ¥250,000 |  2015\u0026ndash;2018.    DAASE: Dynamic Adaptive Automated Software Engineering\n EPSRC Program Grant |  #EP/J017515/1 |  Research Fellow |  ¥6,834,903 |  2012\u0026ndash;2019.    Evolutionary Computation for Dynamic Optimisation in Network Environments\n EPSRC |  #EP/K001523/1 |  Research Fellow |  ¥512,325 |  2013\u0026ndash;2017.    Stable Matching Theory in Multiobjective Evolutionary Algorithm based on Decomposition\n Hong Kong GRF |  #11205314 |  proposal drafting and activity scheduling |  HK$692,894 |  2014\u0026ndash;2017.  \r"}),e.add({id:9,href:"/docs/misc/dtec/",title:"IEEE DTEC",section:"Docs",content:"IEEE CIS Task Force 12\r#\r\rTask Force on Decomposition-based Techniques in Evolutionary Computation\rObjectives\r#\r\rAs the name suggests, the basic idea of the decomposition-based technique is to transform the original complex problem into simplified subproblem(s) so as to facilitate the optimization. Decomposition-based techniques have been widely used for solving both single- and multi-objective optimization problems. More specifically, in single-objective optimization, especially for the large-scale scenarios, which consider a tremendous amount of decision variables, the decomposition-based technique contains three aspects: 1) analyzing and understanding the fitness landscape and modularity structure of the underlying problem; 2) decomposing the original complex problem into several loosely coupled or independent subproblems based on the learnt characteristics; 3) using a meta-heuristic to solve these subproblems in a sequential or concurrent manner. As for multi-objective optimization, the decomposition means to decompose the original multi-objective optimization problem into a number of single-objective optimization sub-problems (or simple multi-objective optimization problems) and then uses a meta-heuristic to optimize these sub-problems simultaneously and collaboratively. In this big data era, the decomposition-based techniques used for both single- and multi-objective optimization can be sythesized to address the challenges posed by the curse of dimensionality, i.e., many objectives and large scale variables.\nThe key objective of this task force it to generalize the decomposition-based idea and to promote its related research, including its development, education and understanding of its sub topic areas.\nThe main objectives of the task force can be summarized as follows:\n create an active and healthy community to promote theme areas of decomposition-based techniques make student, researchers, end-users, developers, and consultants aware of the state-of-the-art promote the use of decomposition-based methodologies/techniques and tools organize conferences/workshop with IEEE CIS Technical Co-Sponsorship organize tutorials, workshops and special sessions launch edited volumes, books, and special issues in journals  Anticipated Interests\r#\r\rThis task force will focus on all aspects, including theory, practice and applications, of the decomposition-based technique in evolutionary computation for solving both single-, multi- and many-objective optimization problems. Topics of interest including but are not limited to the following:\n Design of novel weight vector generation methods Development of new decomposition methods Design of novel computational resource allocation strategies Integration of new reproduction operators Investigation of novel mating selection and replacement procedures Understanding of the relationship between subproblems and solutions Development of novel decomposition-based MOEAs Hybridization of dominance- and decomposition-based approaches Incorporation of user-preferences in decomposition-based MOEAs Extension to many-objective optimization problems Extension to constrained multi- and many-objective optimization problems Design of novel methods to analyze and understand the modularity structure Design of novel cooperative coevolution for large-scale optimization problems Theoretical analysis of the decomposition-based methods  On-Going Activities\r#\r\r Special session on Advances in Decomposition-based Evolutionary Multi-­objective Optimization, at the 2021 IEEE Congress on Computational Intelligence (IEEE CEC 2021), organized by Saúl Zapotecas-Martínez, Bilel Derbel, Ke Li and Qingfu Zhang. Workshop on Decomposition Techniques in Evolutionary Optimization (DTEO), at the 2021 Genetic and Evolutionary Computation Conference (GECCO 2021). The deadline is February 4, 2021 Please consider submitting your best work to this workshop and see you in Cancun! Tutorial on Decomposition Multi-Objective Optimization: Current Developments and Future Opportunities, at the 2021 Genetic and Evolutionary Computation Conference (GECCO 2021), organised by Ke Li and Qingfu Zhang.  Past Activities\r#\r\r Special session on Advances in Decomposition-­based Evolutionary Multi-­objective Optimization, at the 2020 IEEE Congress on Computational Intelligence (IEEE WCCI 2020), organized by Saúl Zapotecas-Martínez, Bilel Derbel, Ke Li and Qingfu Zhang. Workshop on Decomposition Techniques in Evolutionary Optimization (DTEO), at the 2020 Genetic and Evolutionary Computation Conference (GECCO 2020). The deadline is March 27, 2020 Please consider submitting your best work to this workshop and see you in Cancun! Tutorial on Decomposition Multi-Objective Optimization: Current Developments and Future Opportunities, at the 2020 Genetic and Evolutionary Computation Conference (GECCO 2020), organised by Ke Li and Qingfu Zhang. Tutorial on Decomposition Multi-Objective Optimization: Current Developments and Future Opportunities, at the 16th International Conference on Parallel Problem Solving from Nature (PPSN XVI), organised by Ke Li and Qingfu Zhang. Special session on Advances in Decomposition-­based Evolutionary Multi-­objective Optimization, at the 2019 IEEE Congress on Evolutionary Computation (IEEE CEC 2019), organized by Saúl Zapotecas-Martínez, Bilel Derbel, Ke Li and Qingfu Zhang. Workshop on Decomposition Techniques in Evolutionary Optimization (DTEO), at the 2019 Genetic and Evolutionary Computation Conference (GECCO 2019). The deadline is March 27, 2019. Please consider submitting your best work to this workshop and see you in Prague! Tutorial on Decomposition Multi-Objective Optimization: Current Developments and Future Opportunities, at the 2019 Genetic and Evolutionary Computation Conference (GECCO 2019), organised by Ke Li and Qingfu Zhang. Workshop on Decomposition Techniques in Evolutionary Optimization (DTEO), at the 2018 Genetic and Evolutionary Computation Conference (GECCO 2018). The deadline is March 27, 2018. Please consider submitting your best work to this workshop and see you in Kyoto! Tutorial on Decomposition Multi-Objective Optimization: Current Developments and Future Opportunities, at the 2018 Genetic and Evolutionary Computation Conference (GECCO 2018), organised by Ke Li and Qingfu Zhang. Special issue on Recent Advances in Evolutionary Multi-Objective Optimization, at the Swarm and Evolutionary Computation journal, organized by Slim Bechikh and Carlos. A. Coello Coello. The deadline is May 30, 2017. You are highly encouraged to submit your best work here! Special session on Advances in Multiobjective Evolutionary Algorithms based on Decomposition, at the IEEE Congress on Evolutionary Computation (IEEE CEC 2017), organized by Anupam Trivedi, Dipti Srinivasan and Qingfu Zhang. Tutorial on Recent Advances in Multi-objective and Many-objective Evolutionary Algorithms, at the IEEE Congress on Evolutionary Computation (IEEE CEC 2017), organized by Anupam Trivedi and Dipti Srinivasan. Plenary Talk on Use of Traditional Optimization Methods in Multiobjective Evolutionary Computation, at the IEEE Congress on Evolutionary Computation (IEEE CEC 2017), delivered by Qingfu Zhang. Tutorial on Advances in Multi-objective Evolutionary Algorithms based on Decomposition, at the Australasian Conference on Artificial Life and Computational Intelligence (ACALCI 2017), organized by Anupam Trivedi and Dipti Srinivasan. Special session on Advances in Decomposition­based Evolutionary Multi­objective Optimization, at the IEEE World Congress on Computational Intelligence (IEEE WCCI 2016), organized by Sa\u0026rsquo;ul Zapotecas Mart\u0026rsquo;inez, Bilel Derbel, Qingfu Zhang and Carlos A. Coello Coello. Tutorial on Decomposition and Cooperative Coevolution Techniques for Large Scale Global Optimization, at the IEEE Congress on Evolutionary Computation (IEEE CEC 2015), organized by Xiaodong Li. Tutorial on Decomposition and Cooperative Coevolution Techniques for Large Scale Global Optimization, Genetic and Evolutionary Computation Conference (GECCO 2014), organized by Xiaodong Li.  Resources\r#\r\r  Repository of the state-of-the-art developments of multi-objective evolutionary algorithm based on decomposition (MOEA/D) can be found from here.\n  Recent survey paper on the developments of MOEA/D:\n A. Trivedi, D. Srinivasan, K. Sanyal, A. Ghosh, A Survey of Multiobjective Evolutionary Algorithms based on Decomposition, IEEE Trans. on Evolutionary Computation, 21(3): 440-462, 2016. A. Santiago, H. Huacuja, B. Dorronsoro, J. Pecero, C. Santillan, J. Barbosa, J. Monterrubio, A Survey of Decomposition Methods for Multi-objective Optimization, Recent Advances on Hybrid Approaches for Designing Intelligent Systems, 453-465, 2014.    Chairs\r#\r\r \rBilel Derbel (Chair), University of Lille, France. \rKe Li (Vice Chair, Founding Chair), Department of Computer Science, University of Exeter, Exeter, UK. \rQingfu Zhang (Vice Chair), Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China.  Members\r#\r\r \rSlim Bechikh, University of Tunis, Tunisia. \rRan Cheng, Southern University of Science and Technology, China. \rKalyanmoy Deb, Michigan State University, USA. \rHisao Ishibuchi, Osaka Prefecture University, Japan. \rYaochu Jin, University of Surrey, UK. \rSam Kwong, City University of Hong Kong, Hong Kong SAR, China. \rXiaodong Li, RMIT University, Australia. \rArnaud Liefooghe, University of Lille, France. \rHui Li, Xi\u0026rsquo;an Jiaotong University, China. \rMiqing Li, University of Birmingham, UK. \rSanaz Mostaghim, Otto von Guericke University of Magdeburg, Germany. \rTapabrata Ray, University of New South Wales, Australia. \rDipti Srinivasan, National University of Singapore, Singapore. \rHiroyuki Sato, University of Electro-communications, Japan. \rKay Chen Tan, City University of Hong Kong, Hong Kong SAR, China. \rKe Tang, Southern University of Science and Technology, China. \rShengxiang Yang, De Montfort University, UK. \rAimin Zhou, East China Normal University, China. \rSaúl Zapotecas, Unidad Cuajimalpa, México.  "}),e.add({id:10,href:"/docs/home/members/",title:"Members",section:"Docs",content:"Members\r#\r\r\r\r\r\rhr.dashed {\rborder-top: 1px dashed #bbb;\r}\r.grid-container {\rdisplay: grid;\rgrid-template-columns: 15% 35% 15% 35%;\rgrid-gap: 15px;\rbackground-color: transparent;\rpadding: 5px;\ralign-content: left;\r}\r\r\rLookup table of acronyms in this page.\r\u0026#43;\r\r EMO: evolutionary multi-objective optimization BO: Bayesian optimization BLO: bi-level optimization MFO: multi-fidelity optimization CO: constrained optimization DO: dynamic optimization MCM: Monte Carlo method  \r LA: landscape analysis TL: transfer learning MTL: multi-task learning LL: life-long learning GP: Gaussian processes NLP: natural language processing SE: software engineering VNF: virtual network functions  \r\r\r\r\r  Faculty\r#\r\r\rDr Ke Li\nUKRI FLF | Turing Fellow\nSenior Lecturer\nDepartment of Computer Science\nUniversity of Exeter\n     \r\r\r  Research Staff(s)\r#\r\r\rDr Jiangjiao Xu\nResearch Fellow\n07/2020 – present\nInterests: SG, TL, GP, EMO, meta-learning\r\r\rDr Fan Li\nResearch Fellow\n04/2021 – present\nInterests: MFO, TL, MTL, EMO\r\r\rDr Xinqi Li\nResearch Fellow\n11/2021 – present\nInterests: MFO, TL, MTL, EMO\r\r\r  PhD Student(s)\r#\r\r\rPhoenix Williams\nPhD Student\n2020 - present\nInterests: BO, TL, EMO\r\r\rHeng Yang\nPhD Student\n2021 – present\nInterests: NLP, SE, Code naturalness\r\r\r\rPeili Mao\nPhD Student\n2021 – present\nInterests: SE, BO, MCM, LA\r\r\rShengbo Wang\nPhD Student\n2021 – present\nInterests: BO, SO\r\r\r\rJiancheng Qian\nPhD Student\n2020 – present\nInterests: NLP, SE, Code naturalness\r\r\rSavas Yuec\nPhD Student\n2018 - present\nInterests: EMO, BO, reliability\r\r\r\rDongya Wang\nPhD Student\n2018 - present\nInterests: EMO, CO\r\r\r  Research Assistant(s)\r#\r\r\rMaja Kaczmarek\nREP Student\nUniversity of Manchester\n08/2021 – present\nInterests: DL, Bioinformatics\r\r\r  Visitor(s)\r#\r\r\rJiaxin Chen\nVisiting PhD Student\nNortheastern University\n06/2021 – present\nInterests: BLO, EMO, MTL\r\r\rHuan Zhang\nVisiting PhD Student\nNortheastern University\n06/2021 – present\nInterests: DO, TL, EMO\r\r\r  Alumni\r#\r\rPhD student\r  Dr Joseph Billingsly\n 01/09/2017 \u0026ndash; 01/05/2022, University of Exeter PhD student, co-supervised with Prof. Geyong Min Job: -\u0026gt; G-Research London Outputs: 2 EMO papers, 1 GLOBECOM paper, 1 WCCI paper    Dr Mengyuan Wu\n 01/09/2014 \u0026ndash; 01/07/2018, City University of Hong Kong PhD student, co-supervised with Prof. Sam Kwong Job: -\u0026gt; Huawei Noah\u0026rsquo;s Ark Laboratory Outputs: 3 IEEE Trans papers, 1 GECCO paper, 1 SMC paper    Dr Renzhi Chen\n 01/09/2013 \u0026ndash; 01/06/2018, University of Birmingham PhD student, co-supervised with Prof. Xin Yao Job: -\u0026gt; National Defense University of Technology -\u0026gt; PLA Academy of Military Science Outputs: 4 IEEE Trans papers    Visiting student\r Mr Jiadi Liu  01/11/2019 \u0026ndash; 05/11/2020, Southwest University Visiting PhD student   Mr Ruxin Zhao  01/11/2019 \u0026ndash; 05/11/2020, Nanjing University of Science and Technology Visiting PhD student   Mr Yingying Guan  01/11/2019 \u0026ndash; 05/11/2020, Northeastern University Visiting PhD student   Mr Lingjie Li  01/09/2019 \u0026ndash; 01/12/2019, Shenzhen University Visiting student   Dr Nan Mu  01/06/2018 \u0026ndash; 31/12/2018, Wuhan University of Technology Visiting PhD student Job: -\u0026gt; Sichuan Normal University    Visiting scholar\r Dr Lei Yang  01/12/2017 \u0026ndash; 01/12/2018, South China Agricultural University Visiting scholar Job: -\u0026gt; South China Agricultural University   Dr Geng Lin  01/07/2018 \u0026ndash; 01/08/2018, Minjiang University Visiting scholar Job: -\u0026gt; Minjiang University    \r"}),e.add({id:11,href:"/docs/home/news/",title:"News",section:"Docs",content:"\r\rNews Archive\r#\r\r2021\r#\r\rDec\r I have been invited to renew my term as an Associate Editor of IEEE Trans. Evolutionary Computation in 2022. Be humbled to serve our best journal in the evolutionary computation community.  Dec 27  Nov\r Our paper \u0026ldquo;Distributed UAV Swarm Formation and Collision Avoidance Strategies Over Fixed and Switching Topologies\u0026rdquo; is accepted to IEEE Trans. Cybernetics. Congratulations to Jia!  Nov 24  Oct\r  I am proud to be recognized in the Top 2% Scientists Worldwide in a latest study from Stanford University.  Oct 28\n  We are pleased to announce that our collaborative bid with Dr Hao Wang at Leiden University has been funded by Europe Network Fund 2021-22. Looking forward to have many exciting collaboration therein!  Oct 20\n  We are pleased to announce that our collaborative bid with UTS in Australia has been funded by Royal Society under the International Exchange scheme. Looking forward to have many exciting collaboration therein!  Oct 7\n  Sep\r  Our paper \u0026ldquo;Posterior Decision-Making Based on Decomposition-Driven Knee Point Identification\u0026rdquo; is accepted to IEEE Trans. Evolutionary Computation. Congratulations to Haifeng!  Sep 15\n  It is honored to join Alan Turing Institute as a Alan Turing Fellow. I am looking forward new exciting opportunities in this UK\u0026rsquo;s national institute for data science and artificial intelligence.  Sep 3\n  Collaborating with some colleagues, we have established a new theme focused on Trustworthy AI in Institute of Data Science and Artificial Intelligence at Exeter. We are exciting to promote many follow-up activates associated with this theme in due course.  Sep 1\n  After four days intensive talks from our prestigious speakers, we are pleased to announce that our first IEEE CIS Summer School on Data-Driven Artificial/Computational Intelligence is successfully finished. Related materials will be released in due course after the permission from our speakers and IEEE.  Sep 1\n  Aug\r  Our paper \u0026ldquo;Neural Architecture Search for Portrait Parsing\u0026rdquo; is accepted to IEEE Trans. Neural Networks and Learning Systems. Congratulations to our fantastic collaborators!  Aug 11\n  We have four papers accepted by IEEE SMC 2021. Congratulations to Renzhi, Phoenix and Jiangjiao!  Aug 4\n  One proposal get funded by Hong Kong GRF. Congratulations to Prof. Kay Chen Tan and looking forward to working with you soon.  Aug 1\n  Jun\r  Our paper on \u0026ldquo;Transfer Learning Based Parallel Evolutionary Algorithm Framework for Bi-level Optimization\u0026rdquo; is accepted to IEEE Trans. Evolutionary Computation. Congratulations to Lei and collaborators!  Jun 30\n  I have been identified as 2% top scientists in the world across all disciplines from a report published by Stanford University. At the moment, I am ranked around #3200 in the AI subfield worldwide.  Jun 20\n  We are pleased to announce that we will host a summer school on Data-Driven Artificial/Computational Intelligence: Theory and Applications this August. This summer school is kindly supported by IEEE Computational Intelligence Society.  Jun 17\n  We are pleased to announce that Heng has successfully secured his PhD studentship from both CSC and Exeter. He will start his PhD journal in our lab since this autumn.  Jun 3\n  May\r  We are pleased to announce that Peili has successfully secured his studentship and will start his PhD journal since this autumn.  May 28\n  We are glad to announce that Maja Kaczmarek, a second year undergraduate student from the University of Manchester, has been awarded a REP studentship from SWBio DTP (only two in Exeter). We will work with Dr Yiliang Ding\u0026rsquo;s group from John Innes Centre on some exciting topics about RNA structure prediction.  May 17\n  Apr\r  Proud to be one of three awardees in the UK to win an Amazon Research Awards 2020. Really appreciate the generous support from Amazon to carry on our interesting work on multi-task learning! Please see press news from [\rAmazon Science 1], [\rAmazon Science 2], [\rAbout Amazon Blog UK], [\rExeter News], [\rLinkedin], [\r机器之心], [\r新智元], [\rAI科技评论].  Apr 29\n  Our paper \u0026ldquo;A Vector Angles-based Many-objective Particle Swarm Optimization Algorithm Using Archive\u0026rdquo; is accepted to Applied Soft Computing. Congratulations to Lei!  Apr 27\n  Our paper \u0026ldquo;Vertical Distance Based Clonal Selection Mechanism for Multi-objective Immune Algorithm\u0026rdquo; is accepted to Swarm and Evolutionary Computation. Congratulations to Lingjie!  Apr 14\n  Our paper on empirical study of various control mechanisms in interactive evolutionary multi-objective optimization is accepted to IEEE CEC 2021. Congratulations to Guiyu!  Apr 6\n  Mar\r  It is grateful to receive an unrestricted gift from Amazon Research Awards for multi-task learning semantic feature from various injections to test Web application firewalls. Thanks for Amazon\u0026rsquo;s generous support!  Mar 24\n  After a wonderful three-year journey of founding and serving as the Chair of the IEEE Task Force 12 on Decomposition-based Techniques in Evolutionary Computation, I have stepped down as a Vice Chair since this year. It is my pleasure to witness the growing up of our community and a wider range of engagements worldwide. My colleague Prof. Bilel Derbel kindly takes up the new Chair position and will promote the activities further.  Mar 4\n  Jan\r I have been invited to renew my role as an Associate Editor for the prestigious IEEE Trans. Evol. Comput., the flagship journal in artificial/computational Intelligence.  Jan 3  \r"}),e.add({id:12,href:"/docs/research/readinggroup/",title:"Reading Group",section:"Docs",content:"Reading Group\r#\r\r\r\r\rThe paper reading group meets weekly during the semester to discuss papers. Participation is open to all, guests are always welcome; if you are interested in receiving invitations contact the organizer.\r\r\rMore about our reading group culture\r\u0026#43;\r\rEach week we will discuss a different paper. The paper to discuss is announced about one week in advance by the organizer. All participants are expected to read the paper before the meeting. It is recommended to take notes about insights, questions, and other points potentially worth discussing.\nThe goals of the reading group are:\n Critical reflection on scientific work Practice of reading and argumentation strategies Exposure to a broad range of research topics Practice of leading group discussions  The discussion is limited to one hour. The discussion is lead by a moderator, who may also set a focus for the discussion. The moderator will kick off the meeting by giving a short summary of the paper and raising a few points for discussion. The moderator should try to incorporate all participants into the discussion. The moderator role rotates through all participants. The moderator is encouraged to help with the selection of a paper that week.\n\r\r\rAgenda (Year 2021)\r#\r\rJune\r  Learning from Multiple Cities: A Meta-Learning Approach for Spatial-Temporal Prediction\nWWW 2019\n paper |  slides |  Jiangjiao Xu |  June 20\n  Bayesian temporal factorization for multidimensional time series prediction\nIEEE PAMI 2021\n paper |  slides |  Jiangjiao Xu |  June 13\n  Discrete Fourier Transform and Random Fourier Features\nData-Driven Science and Engineering, Chapter 2: 47\u0026ndash;83, 2019\n slides |  Phoenix Williams |  June 9\n  May\r  Global Optimization via Inverse Distance Weighting and Radial Basis Functions\nComput. Optim. Appl., 77: 571\u0026ndash;595, 2020\n paper |  slides |  Jiahao Zhou |  May 30\n  Sequential Monte Carlo: Theory and Applications\nFoundations and Trends in Machine Learning, 12(3): 307\u0026ndash;392, 2019\n paper |  slides |  Peili Mao |  May 23\n  Mar\r  Generalized Homotopy Approach to Multiobjective Optimization\nJ. Optimiz. Theory App., 110: 557\u0026ndash;583, 2001\n paper |  slides |  Yongqi Feng |  Mar 15\n  Machine Learning Approaches in Programming Language Type Inference\n slides |  Jiancheng Qian |  Mar 7\n  Feb\r Understanding and Improving Information Transfer in Multi-Task Learning\nICLR 2020\n paper |  slides |  Renzhi Chen |  Feb 7  Jan\r  OOD-MAML: Meta-Learning for Few-ShotOut-of-Distribution Detection and Classification\nNIPS 2020\n paper |  slides |  Jiangjiao Xu |  Jan 31\n  Domination Measure: A New Metric for Solving Multiobjective Optimization\nINFORMS J. Comput., 32(3): 565\u0026ndash;581, 2020\n paper |  slides |  Xinyu Shan |  Jan 24\n  ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\nICLR 2020\n paper |  slides |  Shasha Zhou |  Jan 3\n  \r Previous reading group information can be found from our archive.\n"}),e.add({id:13,href:"/docs/research/supp/supp_dmi/",title:"Supplementary",section:"Docs",content:"Supplementary Materials\r#\r\rThis website maintains the supplementary materials related to the following paper:\nKe Li, Renzhi Chen, \u0026ldquo;Batched Data-Driven Evolutionary Multi-Objective Optimization Based on Manifold Interpolation\u0026rdquo;, IEEE Trans. Evol. Comput., accepted for publication, 2022.\r\rIt consists of the following parts:\n Appendix document of this paper can be found from this Dropbox link. Tables of all comparison results can be found in this Dropbox link. Figures of all comparison results of the population distributions of on different benchmark test problems can be found in this Dropbox link.  Please cite the paper by using the following bibtex.\n@article{LiC22,\rauthor = {Ke Li and\rRenzhi Chen},\rtitle = {Batched Data-Driven Evolutionary Multi-Objective Optimization Based on Manifold Interpolation},\rjournal = {{IEEE} Trans. Evol. Comput.},\rpages = {1--15},\ryear = {2022},\rnote = {accepted for publication}\r} "}),e.add({id:14,href:"/docs/research/activities/exeter_leiden_symposium/",title:"Symposium",section:"Docs",content:"Exeter-Leiden Symposium on Data-Driven Modeling and Optimization\r#\r\r\r\r\r\rfunction toggle_visibility(id) {\rvar e = document.getElementById(id);\rif(e.style.display == 'block')\re.style.display = 'none';\relse\re.style.display = 'block';\r}\r\r\rhr.dashed {\rborder-top: 1px dashed #bbb;\r}\r.grid-container {\rdisplay: grid;\rgrid-template-columns: 40% 60%;\rgrid-gap: 5px;\rbackground-color: transparent;\rpadding: 5px;\r}\r.bibtexpre{\rbackground-color:#F8F8FA;\rborder-radius:5px;\rborder: 2px solid #D8D8DA;\rmargin:10px; white-space: pre-wrap; word-wrap: break-word; white-space: -moz-pre-wrap; white-space: -pre-wrap;\rwhite-space: -o-pre-wrap;\r}\r\r Objectives\r#\r\rData has been playing an ever-growing role in artificial/computational intelligence. Such role goes beyond its typical use in neural networks and learning systems, encompassing also evolutionary and other meta-heuristic optimization algorithms. The objective of this symposium is to provide a unique and vibrant cohort for sharing and experiencing the emerging methodologies and applications of data-driven artificial/computational intelligence. It will offer keynotes, invited lectures and discussion groups given by experts from Exeter, Leiden and other high-profile institutions. It will provide a unique opportunity for participants to 1) learn about artificial/computational intelligence approaches and their applications; 2) interact with world-renowned experts in computational intelligence; and 3) communicate with experts and peers with a broad range of backgrounds to exchange ideas and form new collaborations.\nIt will take place as part of the outreach activities of the Alan Turing Institute and the Institute for Data Science and Artificial Intelligence (AI) at the University of Exeter. Both institutes are actively developing and fostering a culture of effective interactions for promoting data science and AI for addressing global challenges across disciplines.\n  Venue and Date\r#\r\rThis symposium will be running fully online via Zoom.\n Zoom ID: 931 5551 9825 | PWD: 959476\n June 6, 2022\n  Speakers\r#\r\r \rProf. Michael Emmerich, Leiden University, Netherlands \rProf. Yaochu Jin, University of Bielefeld, Germany\n \rProf. Thomas Bäck, Leiden University, Netherlands \rProf. Juergen Branke, University of Warwick, UK \rProf. Kaisa Miettinen, University of Jyväskylä, Finland    Schedule\r#\r\rNote that the time goes with Central European Time (CET)\r\rThe symposium will be organized in one day with the following agenda.\n 9:00 – 9:10: Introduction talk by Ke Li/Hao Wang    9:10 – 10:00 | Prof. Michael Emmerich | Leiden University, Netherlands\n Lipschitz Models versus Gaussian Process Models in Data-Driven Multi-objective Optimization\n Bio Michael Emmerich is a Germany-born Computer Scientist who currently lives in Finland and in The Netherlands. Since 2016 he is appointed as Associate Professor at Leiden University, The Netherlands, where he leads the Multicriteria Optimization and Decision Analytics Group and since 2019 he is a visiting researcher at Jyvaskyla University, Finland in the Multiobjective and Industrial Optimization Group. He also is Lead AI Scientist at SILO.ai, a provider of AI solutions n the Nordic Countries. He has received his Doctorate in Natural Sciences from the Technical University of Dortmund on the topic of Gaussian Processes for Surrogate-Assisted Multiobjective Design Optimization (2005) under the supervision of Prof. Dr. Ing. H.-P. Schwefel. He also worked as a visiting fellow at the Center for Applied Systems Analysis, ICD e.V. Dorttmund, Institut für Erstarrung unter Schwerelosigkeit e.V. (Aachen), Institute for Fundamental Research of Matter (FOM) Amsterdam, Unversity of the Algarve, IST Lisbon, and Jyvaskyla University. His main contributions are in the field of indicator-based and Bayesian multiobjective optimization, and application of multiobjective optimization in chemical engineering and architectural design.\r |  Abstract The quantification of uncertainty is an important topic when it comes to modeling function landscapes based on previously evaluated input-output pairs. Gaussian process regression and the closely related Kriging method form allegedly the most well known class of surrogate models supporting uncertainty quantification. Here the uncertainty stems from the assumption that outputs are correlated by means of a distance (in the input-space) dependent correlation function. Such knowledge can be used to compute probabilistic confidence bounds to quantify uncertainty of predictions. Lipschitz continuity (and the more general Hölder continuity) on the other hand makes assumptions about bounded change rates. It, too, is based on distances in the input space but its model assumptions yield deterministic upper and lower bounds for the uncertainty ranges in prediction tasks.\nWe contrast these two techniques and reveal commonalities and differences and also comment on their usefulness in the integration to (multiobjective) Bayesian optimization frameworks. A special focus will be on variants of expected improvements and show that the use of a Lipschitzian interpretation of the Expected Improvement is almost equivalent to Shubert's algorithm Computations in the Lipschitzian case are far easier and more efficient while many of the interesting properties of Gaussian process models are preserved.  |  slides |  video    10:00 – 10:50 | Prof. Yaochu Jin | University of Bielefeld, Germany\n Privacy-Preserving Data-driven Evolutionary Optimization\n Bio Yaochu Jin is an Alexander von Humboldt Professor for Artificial Intelligence endowed by the German Federal Ministry of Education and Research, Faculty of Technology, Bielefeld University, Germany. He is also a Distinguished Chair in Computational Intelligence, Department of Computer Science, University of Surrey, Guildford, U.K. He was a \"Finland Distinguished Professor\", University of Jyväskylä, Finland, \"Changjiang Distinguished Visiting Professor\", Northeastern University, China, and \"Distinguished Visiting Scholar\", University of Technology Sydney, Australia. His main research interests include evolutionary optimization, evolutionary learning, trustworthy machine learning, and evolutionary developmental systems.\nProf Jin is presently the Editor-in-Chief of Complex \u0026 Intelligent Systems. He was an IEEE Distinguished Lecturer and the Vice President for Technical Activities of the IEEE Computational Intelligence Society. He is named by the Web of Science as \"a Highly Cited Researcher\" from 2019 to 2021 consecutively. He is a Member of Academia Europaea and Fellow of IEEE.\r |  Abstract Data-driven optimization has received increased interest over the past decade due to its practical importance in many industrial sectors and scientific research fields. Little attention, however, has been paid to privacy preservation the data used for optimization. This talk presents our recent work on privacy-preserving data-driven evolutionary optimization based on federated learning techniques and infill criteria. The prosed framework is applicable to both single- and multi-objective optimization.\r |  slides |  video    10:50 – 11:40 | Prof. Thomas Bäck | Leiden University, Netherlands\n Evolutionary Computation meets Algorithm Configuration (and Applications)\n Bio Thomas Bäck is Full Professor of Computer Science at the Leiden Institute of Advanced Computer Science (LIACS), Leiden University, The Netherlands, where he is head of the Natural Computing group since 2002. He received his PhD (adviser: Hans-Paul Schwefel) in computer science from Dortmund University, Germany, in 1994, and then worked for the Informatik Centrum Dortmund (ICD) as department leader of the Center for Applied Systems Analysis. From 2000 - 2009, Thomas was Managing Director of NuTech Solutions GmbH and CTO of NuTech Solutions, Inc. He gained ample experience in solving real-life problems in optimization and data mining through working with global enterprises such as BMW, Beiersdorf, Daimler, Ford, Honda, and many others.\nThomas has more than 300 publications on natural computing, as well as two books on evolutionary algorithms: Evolutionary Algorithms in Theory and Practice (1996), Contemporary Evolution Strategies (2013). He is co-editor of the Handbook of Evolutionary Computation, and the Handbook of Natural Computing, and co-editor-in-chief of Springer's Natural Computing book series. He is also editorial board member and associate editor of a number of journals on evolutionary and natural computing. Thomas received the best dissertation award from the German Society of Computer Science (Gesellschaft für Informatik, GI) in 1995 and the IEEE Computational Intelligence Society Evolutionary Computation Pioneer Award in 2015.\r |  Abstract Direct global optimization algorithms based on evolutionary computation have shown big successes in a wide range of application domains, for example engineering design optimization.\nIn machine learning, the optimization of hyperparameters (also called the algorithm configuration problem) is an important task. I will briefly explain this problem and provide some examples illustrating that this task can be handled by direct global optimization algorithms as well. While algorithm configuration is commonly applied to machine learning algorithms, algorithm configuration for evolution strategies is also an exciting application domain. I will give a simple example how a combinatorial design space of 4608 configuration variants of evolution strategies can be explored and investigated using data mining. This approach provides an opportunity for discovering the unexplored areas of the optimization algorithm design space. Conversely, direct global optimization methods can also be used as algorithm configurators, or even for addressing the combined algorithm selection and hyperparameter optimization (CASH) task in machine learning. I will provide some insight into research in this direction, too.\nTo conclude, I return to real world application examples and illustrate a few of those that my group worked on, over the past more than 20 years.\r |  slides |  video    11:40 – 14:00: Lunch break    14:00 – 14:50 | Prof. Juergen Branke | University of Warwick, UK\n Bayesian Optimisation and Input Uncertainty Reduction\n Bio Juergen Branke is Professor of Operational Research and Systems at Warwick Business School, University of Warwick (UK). His main research interests include metaheuristics and Bayesian optimisation applied to problems under uncertainty, such as simulation optimisation, dynamically changing problems, and multi-objective problems. Prof. Branke is Editor of ACM Transactions on Evolutionary Learning and Optimization, Area Editor of the Journal of Heuristics and the Journal on Multi-Criteria Decision Analysis, as well as Associate Editor of IEEE Transactions on Evolutionary Computation and the Evolutionary Computation Journal.  |  Abstract Simulators often require calibration inputs estimated from real world data and the estimate can significantly affect simulation output. Particularly when performing simulation optimisation to find an optimal solution, the uncertainty in the inputs significantly affects the quality of the found solution. One remedy is to search for the solution that has the best performance on average over the uncertain range of inputs yielding an optimal compromise solution. We consider the more general setting where a user may choose between either running simulations or instead querying an external data source, improving the input estimate and enabling the search for a more targeted, less compromised solution. We explicitly examine the trade-off between simulation and real data collection in order to find the optimal solution of the simulator with the true inputs. Using a value of information procedure, we propose a novel unified simulation optimisation procedure called Bayesian Information Collection and Optimisation (BICO) that, in each iteration, automatically determines which of the two actions (running simulations or data collection) is more beneficial. Numerical experiments demonstrate that the proposed algorithm is able to automatically determine an appropriate balance between optimisation and data collection.\r |  slides |  video    14:50 – 15:40 | Prof. Kaisa Miettinen | University of Jyväskylä, Finland\n Perspectives to Data-driven Multiobjective Optimization with Interactive Methods\n Bio Kaisa Miettinen is Professor of Industrial Optimization at the University of Jyvaskyla. Her research interests include theory, methods, applications and software of nonlinear multiobjective optimization including interactive and evolutionary approaches. She heads the Research Group on Multiobjective Optimization and is the director of the thematic research area called Decision Analytics utilizing Causal Models and Multiobjective Optimization (DEMO, www.jyu.fi/demo). She has authored over 200 refereed journal, proceedings and collection papers, edited 18 proceedings, collections and special issues and written a monograph Nonlinear Multiobjective Optimization. She is a member of the Finnish Academy of Science and Letters, Section of Science and has served as the President of the International Society on Multiple Criteria Decision Making (MCDM). She belongs to the editorial boards of seven international journals and the Steering Committee of Evolutionary Multiobjective Optimization. She has previously worked at IIASA, International Institute for Applied Systems Analysis in Austria, KTH Royal Institute of Technology in Stockholm, Sweden and Helsinki School of Economics, Finland. She has received the Georg Cantor Award of the International Society on MCDM for independent inquiry in developing innovative ideas in the theory and methodology.  |  Abstract In data analytics, we can use descriptive analytics to understand the data or predictive analytics to make predictions, but to know what actions to take to reach desired outcomes, we need prescriptive analytics. To make optimized recommendations or decisions based on the data, we can fit models in the data and derive optimization problems. In many cases, real decisions to be made are characterized by multiple conflicting objectives to be optimized and we can support decision making by applying appropriate multiobjective optimization methods. This we can call decision analytics.\nIn this talk, I discuss different elements of a seamless chain from data to data-driven decision support involving multiobjective optimization. Eventually, the derived multiobjective optimization problem is solved with an appropriate interactive method. In that way, the decision maker with domain expertise can augment information contained in the data and direct the solution process with one’s preferences. At the same time, the decision maker gains insight into the interdependencies and trade-offs among the conflicting objectives, and can get convinced of the quality of the most preferred solution. In addition, I give some examples of data-driven decision making problems. Finally, I give an overview of the modular, open-source software framework DESDEO containing different interactive methods.\r |  slides |  video    15:40 – 16:55\n Panel discussion    16:55 – 17:00: Closing remark    Sponsors\r#\r\rWe are grateful to the support from UKRI Future Leaders Fellowship (MR/S017062/1), European Network Fund@Exeter (No. GP ENF5.10) and Turing Fellowship.\n"}),e.add({id:15,href:"/docs/home/vacancies/",title:"Vacancies",section:"Docs",content:"Vacancies\r#\r\r\r\rWe very much welcome and support applications from underrepresented groups and from people of all shapes, shades, and genders. If you are uncertain about your possible role in our team, please do not hesitate to get in touch with Ke Li.\nPhD\rThere are multiple opportunities available at the University of Exeter to fund your PhD study.\n China Scholarship Council (Chinese Students): The University of Exeter is proud to offer up to 50 full-time PhD scholarships in collaboration with the China Scholarship Council (CSC). The application deadline is usually around the beginning of January. More detailed information can be found from here. Doctor Training Partnerships (UK/EU \u0026amp; International Students): As a research intensive university in the UK, Exeter has strong commitments to work in partnerships with several research councils to fund talented students for a PhD. The deadline is usually around the end of January every year. More detailed information can be found from here.  Research Fellow\rWe welcome applications by post-doc and research fellows. Unfortunately, we do not have funding available at this time. However, if you want to propose a collaboration based on an externally-funded post-doctoral scholarship, please get in touch. Here are some potential funding opportunities:\n \rDorothy Hodkin Fellowship \rMarie-Curie Fellowship \rEPSRC Post-Doctoral Fellowship \rNewton International Fellowships (for who are not based in the UK) \rUniversity Research Fellowship \rRoyal Commission for the Exhibition of 1851 Research Fellowship \rRoyal Academy of Engineering Fellowship  Visiting Scholar\rWe are also welcome the self-funded visiting scholars (supported by CSC or other government organisations) to join the COLA Laboratory. Note that I prefer at most host 3 long-term (at least one year) visitors simultaneously in order to allocate enough supervision time.\r\rSome useful tips to know when preparing your pitching email\r\u0026#43;\r\r Use Subject Line: 20** Prospective Students/Postdoc/Visiting Scholar_YourName Briefly introduce your education background, GPA/Rank, IELTS/TOEFL score, and other highlights (e.g., publications, previous awards and industrial experiences). Briefly (1-page summary) introduce your research experience, and explain what your general research interests and strengths are. Explain why you want to choose our lab. Briefly mention one of my recent work that you are most interested in, and discuss the potential extensions that you might want to work on. Include your CV in the attachment.  Due to the large volume of inquiries, I cannot unfortunately afford to reply all those enquiry emails.\r\r\r\r We are not able to host any summer intern in our lab at the moment.\n "})})()